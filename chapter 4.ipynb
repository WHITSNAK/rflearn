{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIONS = ['up', 'down', 'left', 'right']\n",
    "\n",
    "\n",
    "class GridWorld:\n",
    "    ACTION_MAP = {\n",
    "        'up': (-1, 0),\n",
    "        'down': (1, 0),\n",
    "        'left': (0, -1),\n",
    "        'right': (0, 1),\n",
    "    }\n",
    "\n",
    "    def __init__(self, nrow, ncol):\n",
    "        self.nrow = nrow\n",
    "        self.ncol = ncol\n",
    "        self.shape = (nrow, ncol)\n",
    "        self.grid = [[0]*ncol for _ in range(nrow)]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for cell in product(range(self.nrow), range(self.ncol)):\n",
    "            yield cell\n",
    "    \n",
    "    def is_terminal(self, state):\n",
    "        return state in [(0, 0), (self.nrow-1, self.ncol-1)]\n",
    "\n",
    "    def get_move(self, action):\n",
    "        return self.ACTION_MAP[action.lower()]\n",
    "\n",
    "    def get_reward(self, state):\n",
    "        if self.is_terminal(state):\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "    \n",
    "    def get_new_state(self, state, move):\n",
    "        _row = min(max(state[0] + move[0], 0), self.nrow-1)\n",
    "        _col = min(max(state[1] + move[1], 0), self.ncol-1)\n",
    "        new_state = (_row, _col)\n",
    "        return new_state\n",
    "    \n",
    "    def get_four_neighbors(self, state):\n",
    "        return [\n",
    "            self.get_new_state(state, self.get_move('up')),\n",
    "            self.get_new_state(state, self.get_move('down')),\n",
    "            self.get_new_state(state, self.get_move('left')),\n",
    "            self.get_new_state(state, self.get_move('right')),\n",
    "        ]\n",
    "\n",
    "    def step(self, state, action):\n",
    "        move = self.get_move(action)\n",
    "        new_state = self.get_new_state(state, move)\n",
    "        reward = self.get_reward(new_state)\n",
    "        return new_state, reward\n",
    "    \n",
    "\n",
    "\n",
    "def random_policy(state):\n",
    "    \"\"\"Uniform policy across action space\"\"\"\n",
    "    return np.ones(len(ACTIONS)) / len(ACTIONS)\n",
    "\n",
    "\n",
    "def evaluate_cell_mc(grid, cell, policy, iter):\n",
    "    V = []  \n",
    "    for _ in range(iter):\n",
    "        s0 = cell\n",
    "        R = 0\n",
    "        while True:\n",
    "            if grid.is_terminal(s0):\n",
    "                break\n",
    "\n",
    "            action = np.random.choice(ACTIONS, p=policy(s0))\n",
    "            s1, r = grid.step(s0, action)\n",
    "            s0 = s1\n",
    "            R += r\n",
    "        V.append(R) # value per step\n",
    "        \n",
    "    return np.mean(V)\n",
    "\n",
    "\n",
    "def evaluate_grid_mc(grid, policy, iter=2000):\n",
    "    \"\"\"Monte Carlo evaluation of policy value\"\"\"\n",
    "    vs = []\n",
    "    for cell in grid:\n",
    "        col_v = evaluate_cell_mc(grid, cell, policy, iter=iter)  \n",
    "        vs.append(col_v)\n",
    "    \n",
    "    return np.array(vs).reshape(grid.shape)\n",
    "\n",
    "\n",
    "def evaluate_grid_pi(grid, policy, delta=0.001):\n",
    "    \"\"\"Policy evaluation using bellman equation updates\"\"\"\n",
    "    V = deepcopy(grid.grid)\n",
    "    delta = 0.001\n",
    "    while True:  # iterations until error less than delta\n",
    "        V2 = deepcopy(V)\n",
    "        _delta = 0\n",
    "        for s0 in grid:  # calculate value each cell\n",
    "            if grid.is_terminal(s0):\n",
    "                V2[s0[0]][s0[1]] = 0\n",
    "                continue\n",
    "\n",
    "            neighbors = grid.get_four_neighbors(s0)\n",
    "\n",
    "            # update using previous iteration\n",
    "            rs = [grid.get_reward(s0) for _ in neighbors]  # reward on current cell\n",
    "            vs = [V[s1[0]][s1[1]] for s1 in neighbors]  # values from all actions\n",
    "            pis = policy(s0)\n",
    "            v = np.sum(np.c_[rs, vs].sum(1) * pis)\n",
    "            V2[s0[0]][s0[1]] = v\n",
    "\n",
    "            _delta = max(_delta, abs(v - V[s0[0]][s0[1]]))\n",
    "\n",
    "        V = deepcopy(V2)\n",
    "        if _delta < delta:\n",
    "            break\n",
    "\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -14., -20., -22.],\n",
       "       [-14., -18., -20., -20.],\n",
       "       [-20., -20., -18., -14.],\n",
       "       [-22., -20., -14.,   0.]])"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = GridWorld(4, 4)\n",
    "np.round(evaluate_grid_pi(grid, random_policy, delta=0.001), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -13., -19., -22.],\n",
       "       [-13., -17., -20., -19.],\n",
       "       [-19., -19., -17., -13.],\n",
       "       [-21., -20., -13.,   0.]])"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = GridWorld(4, 4)\n",
    "np.round(evaluate_grid_mc(grid, random_policy, iter=2000), 0)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "953e921e75f8b4c26dea58d124160352c5b7006d92053e7a19fa8af8b405b1de"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
